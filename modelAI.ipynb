{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7TZ-a1Tkfaqj"
   },
   "source": [
    "# Version finale"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "fI_RCNAe68qq",
    "jupyter": {
     "is_executing": true
    },
    "ExecuteTime": {
     "start_time": "2024-06-20T22:43:06.445925Z"
    }
   },
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tensorflow.keras.layers import Dense, Conv1D, Flatten\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "import joblib"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "Wfgg78Lk7E2N"
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Load and preprocess the dataset\n",
    "df = pd.read_csv('./all_buildings_data.csv')\n",
    "\n",
    "# Drop unnecessary columns\n",
    "df = df.drop(columns=['Unnamed: 0', 'building'])\n",
    "\n",
    "# Extract features (aggregate power) and targets (appliance power consumption)\n",
    "X = df['aggregate'].values.reshape(-1, 1)\n",
    "y = df.drop(columns=['aggregate']).values  # All appliance columns as targets\n",
    "\n",
    "# Standardize features\n",
    "scaler = StandardScaler()\n",
    "X = scaler.fit_transform(X)\n",
    "joblib.dump(scaler, 'scaler.pkl')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "vwRvOo2y7K8j"
   },
   "outputs": [],
   "source": [
    "\n",
    "# Create windows of data\n",
    "def create_windows(data, targets, window_size):\n",
    "    X = []\n",
    "    y = []\n",
    "    for i in range(len(data) - window_size):\n",
    "        X.append(data[i:i + window_size])\n",
    "        y.append(targets[i + window_size])\n",
    "    return np.array(X), np.array(y)\n",
    "\n",
    "window_size = 60  # 60 points of time in our window\n",
    "\n",
    "# Prepare the data\n",
    "X, y = create_windows(X, y, window_size)\n",
    "\n",
    "# Reshape X for Conv1D input\n",
    "X = X.reshape((X.shape[0], X.shape[1], 1))\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "-saYSQ_T7OLJ"
   },
   "outputs": [],
   "source": [
    "\n",
    "# Define the multi-output Seq2Point model\n",
    "def multi_output_seq2point_model(input_shape, num_outputs):\n",
    "    model = Sequential()\n",
    "    model.add(Conv1D(30, 10, activation='relu', input_shape=input_shape, padding='same'))\n",
    "    model.add(Conv1D(30, 8, activation='relu', padding='same'))\n",
    "    model.add(Conv1D(40, 6, activation='relu', padding='same'))\n",
    "    model.add(Conv1D(50, 5, activation='relu', padding='same'))\n",
    "    model.add(Conv1D(50, 5, activation='relu', padding='same'))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(1024, activation='relu'))\n",
    "    model.add(Dense(num_outputs))  # Output layer for multi-output regression\n",
    "    model.compile(loss='mse', optimizer='adam', metrics=['mae'])\n",
    "    return model\n",
    "\n",
    "    # Create the model\n",
    "num_appliances = y_train.shape[1]\n",
    "model = multi_output_seq2point_model((X_train.shape[1], 1), num_appliances)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ELIc7SGt7TNG",
    "outputId": "8dafe90b-2c40-4013-b017-686d07753503"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "2008/2008 [==============================] - 157s 76ms/step - loss: 4896.4839 - mae: 24.6621 - val_loss: 3758.7856 - val_mae: 20.3081\n",
      "Epoch 2/50\n",
      "2008/2008 [==============================] - 152s 76ms/step - loss: 3862.5725 - mae: 20.9599 - val_loss: 4026.1636 - val_mae: 20.5106\n",
      "Epoch 3/50\n",
      "2008/2008 [==============================] - 152s 76ms/step - loss: 3528.7781 - mae: 19.6298 - val_loss: 3268.3420 - val_mae: 20.3092\n",
      "Epoch 4/50\n",
      "2008/2008 [==============================] - 155s 77ms/step - loss: 3294.0637 - mae: 19.0670 - val_loss: 3104.5808 - val_mae: 19.0075\n",
      "Epoch 5/50\n",
      "2008/2008 [==============================] - 154s 77ms/step - loss: 3012.6169 - mae: 18.1298 - val_loss: 3090.0759 - val_mae: 17.7441\n",
      "Epoch 6/50\n",
      "2008/2008 [==============================] - 154s 77ms/step - loss: 2865.5784 - mae: 17.6248 - val_loss: 3004.3115 - val_mae: 18.0608\n",
      "Epoch 7/50\n",
      "2008/2008 [==============================] - 155s 77ms/step - loss: 2688.3262 - mae: 16.9520 - val_loss: 2783.4927 - val_mae: 16.7150\n",
      "Epoch 8/50\n",
      "2008/2008 [==============================] - 149s 74ms/step - loss: 2588.4216 - mae: 16.6657 - val_loss: 2769.5896 - val_mae: 17.6312\n",
      "Epoch 9/50\n",
      "2008/2008 [==============================] - 149s 74ms/step - loss: 2427.5522 - mae: 16.2442 - val_loss: 2673.6516 - val_mae: 17.3809\n",
      "Epoch 10/50\n",
      "2008/2008 [==============================] - 147s 73ms/step - loss: 2300.4021 - mae: 15.8910 - val_loss: 2720.9443 - val_mae: 15.8727\n",
      "Epoch 11/50\n",
      "2008/2008 [==============================] - 148s 74ms/step - loss: 2225.3276 - mae: 15.8215 - val_loss: 2686.2261 - val_mae: 16.7115\n",
      "Epoch 12/50\n",
      "2008/2008 [==============================] - 146s 73ms/step - loss: 2105.3801 - mae: 15.4870 - val_loss: 3139.1541 - val_mae: 15.9516\n",
      "Epoch 13/50\n",
      "2008/2008 [==============================] - 143s 71ms/step - loss: 2017.2677 - mae: 15.2423 - val_loss: 2757.4482 - val_mae: 15.7764\n",
      "Epoch 14/50\n",
      "2008/2008 [==============================] - 147s 73ms/step - loss: 1887.6482 - mae: 15.0537 - val_loss: 2596.5898 - val_mae: 15.0422\n",
      "Epoch 15/50\n",
      "2008/2008 [==============================] - 144s 72ms/step - loss: 1816.4846 - mae: 14.8488 - val_loss: 2588.0725 - val_mae: 15.3589\n",
      "Epoch 16/50\n",
      "2008/2008 [==============================] - 144s 72ms/step - loss: 1730.8652 - mae: 14.6472 - val_loss: 2813.3923 - val_mae: 15.4502\n",
      "Epoch 17/50\n",
      "2008/2008 [==============================] - 145s 72ms/step - loss: 1662.4912 - mae: 14.5199 - val_loss: 2555.1494 - val_mae: 14.7326\n",
      "Epoch 18/50\n",
      "2008/2008 [==============================] - 143s 71ms/step - loss: 1534.3827 - mae: 14.3168 - val_loss: 2546.3730 - val_mae: 14.8928\n",
      "Epoch 19/50\n",
      "2008/2008 [==============================] - 151s 75ms/step - loss: 1480.3417 - mae: 14.1938 - val_loss: 2425.7607 - val_mae: 14.6194\n",
      "Epoch 20/50\n",
      "2008/2008 [==============================] - 144s 72ms/step - loss: 1414.1841 - mae: 14.0419 - val_loss: 2334.7085 - val_mae: 14.5295\n",
      "Epoch 21/50\n",
      "2008/2008 [==============================] - 148s 74ms/step - loss: 1331.2141 - mae: 13.8708 - val_loss: 2726.2734 - val_mae: 15.0617\n",
      "Epoch 22/50\n",
      "2008/2008 [==============================] - 144s 72ms/step - loss: 1303.8787 - mae: 13.8775 - val_loss: 2456.8667 - val_mae: 15.1486\n",
      "Epoch 23/50\n",
      "2008/2008 [==============================] - 148s 74ms/step - loss: 1208.9070 - mae: 13.6380 - val_loss: 2494.3760 - val_mae: 14.8361\n",
      "Epoch 24/50\n",
      "2008/2008 [==============================] - 149s 74ms/step - loss: 1202.0140 - mae: 13.6385 - val_loss: 2549.5200 - val_mae: 14.8037\n",
      "Epoch 25/50\n",
      "2008/2008 [==============================] - 145s 72ms/step - loss: 1128.5336 - mae: 13.4289 - val_loss: 2315.0540 - val_mae: 14.5465\n",
      "Epoch 26/50\n",
      "2008/2008 [==============================] - 146s 73ms/step - loss: 1051.6892 - mae: 13.2441 - val_loss: 2691.9263 - val_mae: 14.9686\n",
      "Epoch 27/50\n",
      "2008/2008 [==============================] - 145s 72ms/step - loss: 1002.6084 - mae: 13.1003 - val_loss: 2479.0500 - val_mae: 14.6533\n",
      "Epoch 28/50\n",
      "2008/2008 [==============================] - 148s 74ms/step - loss: 992.4456 - mae: 13.1211 - val_loss: 2329.0557 - val_mae: 14.7153\n",
      "Epoch 29/50\n",
      "2008/2008 [==============================] - 147s 73ms/step - loss: 941.8058 - mae: 12.9544 - val_loss: 2323.9407 - val_mae: 14.9149\n",
      "Epoch 30/50\n",
      "2008/2008 [==============================] - 144s 72ms/step - loss: 883.1188 - mae: 12.8037 - val_loss: 2408.8350 - val_mae: 14.3280\n",
      "Epoch 31/50\n",
      "2008/2008 [==============================] - 148s 74ms/step - loss: 876.5028 - mae: 12.8292 - val_loss: 2417.0457 - val_mae: 14.4717\n",
      "Epoch 32/50\n",
      "2008/2008 [==============================] - 147s 73ms/step - loss: 853.7108 - mae: 12.7741 - val_loss: 2288.3115 - val_mae: 14.3767\n",
      "Epoch 33/50\n",
      "2008/2008 [==============================] - 147s 73ms/step - loss: 782.2283 - mae: 12.4940 - val_loss: 2394.4595 - val_mae: 14.1101\n",
      "Epoch 34/50\n",
      "2008/2008 [==============================] - 146s 72ms/step - loss: 764.6183 - mae: 12.4736 - val_loss: 2250.8364 - val_mae: 14.3486\n",
      "Epoch 35/50\n",
      "2008/2008 [==============================] - 145s 72ms/step - loss: 756.6949 - mae: 12.4556 - val_loss: 2295.9824 - val_mae: 14.2273\n",
      "Epoch 36/50\n",
      "2008/2008 [==============================] - 148s 74ms/step - loss: 728.4396 - mae: 12.3168 - val_loss: 2236.5728 - val_mae: 13.9263\n",
      "Epoch 37/50\n",
      "2008/2008 [==============================] - 146s 73ms/step - loss: 695.5822 - mae: 12.1921 - val_loss: 2236.0320 - val_mae: 13.7724\n",
      "Epoch 38/50\n",
      "2008/2008 [==============================] - 146s 73ms/step - loss: 687.7127 - mae: 12.2059 - val_loss: 2275.4883 - val_mae: 13.8213\n",
      "Epoch 39/50\n",
      "2008/2008 [==============================] - 145s 72ms/step - loss: 648.0641 - mae: 11.9733 - val_loss: 2270.4561 - val_mae: 14.4934\n",
      "Epoch 40/50\n",
      "2008/2008 [==============================] - 145s 72ms/step - loss: 654.6027 - mae: 12.0487 - val_loss: 2296.6501 - val_mae: 14.0340\n",
      "Epoch 41/50\n",
      "2008/2008 [==============================] - 147s 73ms/step - loss: 635.0665 - mae: 11.9006 - val_loss: 2178.4541 - val_mae: 13.9338\n",
      "Epoch 42/50\n",
      "2008/2008 [==============================] - 148s 74ms/step - loss: 617.0416 - mae: 11.7946 - val_loss: 2309.3374 - val_mae: 14.0867\n",
      "Epoch 43/50\n",
      "2008/2008 [==============================] - 147s 73ms/step - loss: 596.8577 - mae: 11.7507 - val_loss: 2263.1887 - val_mae: 13.9745\n",
      "Epoch 44/50\n",
      "2008/2008 [==============================] - 147s 73ms/step - loss: 577.8317 - mae: 11.6377 - val_loss: 2243.1077 - val_mae: 14.4416\n",
      "Epoch 45/50\n",
      "2008/2008 [==============================] - 148s 74ms/step - loss: 586.2552 - mae: 11.6673 - val_loss: 2185.1145 - val_mae: 13.7637\n",
      "Epoch 46/50\n",
      "2008/2008 [==============================] - 148s 74ms/step - loss: 542.4415 - mae: 11.4084 - val_loss: 2256.8213 - val_mae: 14.6438\n",
      "Epoch 47/50\n",
      "2008/2008 [==============================] - 151s 75ms/step - loss: 550.8234 - mae: 11.4711 - val_loss: 2187.9382 - val_mae: 13.5989\n",
      "Epoch 48/50\n",
      "2008/2008 [==============================] - 146s 73ms/step - loss: 541.6395 - mae: 11.3520 - val_loss: 2304.1069 - val_mae: 14.0768\n",
      "Epoch 49/50\n",
      "2008/2008 [==============================] - 149s 74ms/step - loss: 534.3015 - mae: 11.3595 - val_loss: 2255.7947 - val_mae: 14.2095\n",
      "Epoch 50/50\n",
      "2008/2008 [==============================] - 149s 74ms/step - loss: 522.6392 - mae: 11.2418 - val_loss: 2172.7390 - val_mae: 13.7944\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x7f45b106ef50>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train the model\n",
    "model.fit(X_train, y_train, epochs=50, batch_size=32, validation_split=0.1, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "qQ4dg3WiJNDq",
    "outputId": "caa0cd25-2fca-46e6-9a43-42015eaa1afc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "558/558 [==============================] - 8s 13ms/step - loss: 2582.9041 - mae: 13.9785\n",
      "Model Mean Absolute Error: 13.978487968444824\n"
     ]
    }
   ],
   "source": [
    "loss, mae = model.evaluate(X_test, y_test, verbose=1)\n",
    "print(f'Model Mean Absolute Error: {mae}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "BHRn_I78Mli_",
    "outputId": "6857cb68-fa3e-4ed6-c311-5179c9421004"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 25ms/step\n",
      "Predicted appliance consumption:\n",
      "laptop computer: 4.010241508483887\n",
      "television: 3.8627007007598877\n",
      "light: 106.19120788574219\n",
      "HTPC: 3.4630422592163086\n",
      "food processor: 0.0\n",
      "toasted sandwich maker: 0.0\n",
      "toaster: 0.0\n",
      "microwave: 0.17969262599945068\n",
      "computer monitor: 2.938211679458618\n",
      "audio system: 3.760430335998535\n",
      "audio amplifier: 6.218283653259277\n",
      "broadband router: 6.184991836547852\n",
      "ethernet switch: 3.972405433654785\n",
      "USB hub: 4.020544052124023\n",
      "tablet computer charger: 0.4330313205718994\n",
      "radio: 106.18269348144531\n",
      "wireless phone charger: 1.45587956905365\n",
      "mobile phone charger: 0.5068104267120361\n",
      "coffee maker: 0.7939611077308655\n",
      "computer: 13.164216041564941\n",
      "external hard disk: 13.138588905334473\n",
      "desktop computer: 5.918463706970215\n",
      "printer: 3.4136481285095215\n",
      "immersion heater: 125.8182144165039\n",
      "security alarm: 126.0098876953125\n",
      "projector: 106.19452667236328\n",
      "server computer: 107.14362335205078\n",
      "running machine: 107.1309585571289\n",
      "network attached storage: 106.22518920898438\n",
      "fridge: 11.596797943115234\n",
      "air conditioner: 106.17343139648438\n"
     ]
    }
   ],
   "source": [
    "# List of appliance names\n",
    "appliances = [\n",
    "    'laptop computer', 'television', 'light', 'HTPC', 'food processor', 'toasted sandwich maker',\n",
    "    'toaster', 'microwave', 'computer monitor', 'audio system', 'audio amplifier', 'broadband router',\n",
    "    'ethernet switch', 'USB hub', 'tablet computer charger', 'radio', 'wireless phone charger', 'mobile phone charger',\n",
    "    'coffee maker', 'computer', 'external hard disk', 'desktop computer', 'printer', 'immersion heater',\n",
    "    'security alarm', 'projector', 'server computer', 'running machine', 'network attached storage', 'fridge',\n",
    "    'air conditioner'\n",
    "]\n",
    "\n",
    "\n",
    "# Function to predict appliance consumption from aggregate\n",
    "def predict_appliance_consumption(aggregate_window, scaler, window_size):\n",
    "    # Standardize the input features\n",
    "    aggregate_window = np.array(aggregate_window).reshape(-1, 1)\n",
    "    aggregate_window = scaler.transform(aggregate_window)\n",
    "    aggregate_window = aggregate_window.reshape((1, window_size, 1))\n",
    "\n",
    "    # Predict using the trained model\n",
    "    predictions = model.predict(aggregate_window)\n",
    "    # Clip negative values to zero\n",
    "    predictions = np.clip(predictions, 0, None)\n",
    "    return predictions\n",
    "\n",
    "# Example usage (a new window of aggregate power consumption data)\n",
    "aggregate_window = [\n",
    "    100, 52.31, 1.0, 135.28, 2.0, 1.0, 1.0, 0.0, 1.0, 4.89, 51.10, 0.0, 6.0,\n",
    "    51.44, 51.44, 50.52, 135.28, 1.0, 0.0, 0.0, 13.88, 13.88, 57.26, 50.93,\n",
    "    102.20, 102.20, 135.28, 135.28, 135.28, 135.28, 0.0, 135.28, 99.8, 98.7,\n",
    "    102.3, 101.2, 100.5, 102.0, 105.0, 99.9, 98.4, 102.1, 101.3, 100.4, 100.7,\n",
    "    102.8, 101.5, 100.3, 101.7, 100.9, 102.2, 100.6, 99.5, 101.4, 100.2, 100.1,\n",
    "    99.6, 98.5, 100.0, 99.7\n",
    "]\n",
    "\n",
    "predictions = predict_appliance_consumption(aggregate_window, scaler, window_size)\n",
    "\n",
    "\n",
    "# Map appliance names to their predicted consumption values\n",
    "predicted_consumption = dict(zip(appliances, predictions[0]))\n",
    "\n",
    "print(\"Predicted appliance consumption:\")\n",
    "for appliance, consumption in predicted_consumption.items():\n",
    "    print(f\"{appliance}: {consumption}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "n2wDZ-I7bhOy",
    "outputId": "5b6cd562-4149-4536-d25f-cc71b00b7e33"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "558/558 [==============================] - 7s 13ms/step\n",
      "Mean Absolute Error (MAE): 13.978487983944513\n",
      "Mean Squared Error (MSE): 2582.9044247016914\n",
      "R-squared (R²): 0.5731571220226468\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "\n",
    "# Predict on the test set\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Calculate evaluation metrics\n",
    "mae = mean_absolute_error(y_test, y_pred)\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "print(f\"Mean Absolute Error (MAE): {mae}\")\n",
    "print(f\"Mean Squared Error (MSE): {mse}\")\n",
    "print(f\"R-squared (R²): {r2}\")\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "name": "python3",
   "language": "python"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
